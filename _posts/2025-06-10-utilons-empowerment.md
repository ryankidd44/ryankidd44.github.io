---
layout: post
title: "Utilons vs. empowerment"
category: note
tags:
  - Philosophy
  - EA
  - X
---

EAs seem to come in two primary flavors:

- Specialists with high cognitive empathy who want to make utilons go up;
- Generalists with high affective empathy who want to empower all beings.

The first type of individual seems principally concerned with the *aesthetics* of doing good better. They look for an authoritative list of the top problems and pick the highest rank. To them, making donations based on heart-wrenching pictures of crying babies is "unaesthetic" and a "baser instinct"; emotion-based decisions are an alien "other" to be excised and examined. "Doing good better" mostly means ((doing better) at good).

The second type of EA generally has higher interoception and acceptance of emotions. They are less concerned with determining the "true moral theory" or singularly optimizing a narrow, mathematically aesthetic measure of flourishing (which is seen as a proxy, not the true goal). Emotions are treated as "the predicates of action" rather than as foreign intruders or biological crossed wires. "Doing good better" means ((doing good) better).

In defence of the first, emotions often *are* weird, hyper-specific reactions to internalized traumas or species-level herd instincts. But I think there's also deep meaning to many emotional triggers (e.g., self-other overlap) and outright rejecting emotional reactions seems a poor strategy for learning from them or constructing a robust moral framework grounded in universal instincts.

Also, without a good, quantifiable definition of ethical delta (e.g., QALYs), it's pretty hard to hill-climb on improving the world; we're basically reduced to guesses and reading vibes, or overly fixating on an easily optimized, inadequate proxy like GDP.

I also think that it's a fool's errand for most people to try "doing the research" to determine what the most impactful charities/careers are, and following your instincts is probably even more misleading; why would our monkey++ brains have good instincts about wicked global problems we never evolved to deal with? 80,000 Hours and GiveWell seem extremely reliable in comparison.

But I refuse to demonise the parts of me that feel deep, visceral emotion at pictures of war or suffering or factory farms. This emotional core feels like the root of any high-minded plan to improve the world or any "will to purpose". Short of some objective morality (which seems implausible/noncognitive), this is the best we get: reconciling our shards of purpose and tangled webs of emotion and meaning into SOMETHING, and striving to be better in its fulfilment.

To that end, making utilons go up and empowering all beings might be the same thing: if every sentient being is the center of its own moral universe and they all have to contend with the others' potential violence, share finite resources, and unite against common enemies (e.g., entropy), certain cooperative, empathetic instincts might fall out of whatever optimization process they are embedded in. I don't pretend this is "objective morality" - arbitrary environments could engender arbitrary goals - but from a deterministic, self-centric perspective, being a nice, cooperative being seems pretty good for me and also everyone else!

So, in summary, neither pole I've described seems complete, if the goal is reconciliation and fulfilment of moral instincts. There are uses for both dispassionate analytics and embodied emotionality. Thank you to EAs for caring about (doing), (good), and (better), in whichever order they choose.
