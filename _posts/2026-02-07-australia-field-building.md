---
layout: post
title: "AI safety field-building in Australia should accelerate"
description: ""
category: note
tags:
  - AI Safety
  - Field Building
---

Originally published on [LessWrong](https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=aPAtazuRt2np2zn6n).

I think that AI safety field-building in Australia should accelerate. My rationale and recommendations are below.

COI: I am Australian and advise AI Safety ANZ and TARA in unpaid roles.

# Rationale

## New frontier AI offices

OpenAI opened a [Sydney office](https://openai.com/global-affairs/openai-for-australia/) in Dec 2025 and Anthropic is [planning to open](https://www.smh.com.au/technology/chatgpt-s-biggest-rival-is-setting-up-shop-in-australia-20260115-p5nu7y.html) a Sydney office in 2026\. These offices may hire safety staff from local talent, or partner with local auditing, evaluation, and security companies, including [Harmony Intelligence](https://www.harmonyintelligence.com/), [Good Ancestors](https://www.goodancestors.org.au/), and [Gradient Institute](https://www.gradientinstitute.org/).

## New AISI

An Australian AISI was [announced](https://www.industry.gov.au/news/australia-establish-new-institute-strengthen-ai-safety) for early 2026 and is [currently hiring](https://www.goodancestors.org.au/aisi-founders). The UK AISI has benefited from close partnerships with [Apollo Research](https://www.apolloresearch.ai/), [METR](https://metr.org/), and the [LISA](https://www.safeai.org.uk/) office community. There is a community space in Sydney, the [Sydney AI Safety Space](https://sydneyaisafetyspace.org/), and two field-building organizations, [AI Safety ANZ](https://www.aisafetyanz.com.au/) and [TARA](https://www.taraprogram.org/), but these could expand substantially.

## Datacenter build-out

Australia seems like a prime location for datacenter build-out. OpenAI published an ["AI blueprint" for Australia](https://openai.com/global-affairs/openais-australia-economic-blueprint/), calling for datacenter build-out, and started building a [$4.6B datacenter](https://www.bloomberg.com/news/articles/2025-12-04/openai-nextdc-plan-to-develop-large-scale-data-center-in-sydney) in Sydney in Dec 2025\. Australia is a [NATO](https://en.wikipedia.org/wiki/NATO) partner, [Five Eyes](https://en.wikipedia.org/wiki/Five_Eyes) member, and member of the [AUKUS](https://en.wikipedia.org/wiki/AUKUS) security partnership with the US and UK; it's much more secure and aligned with US/UK interests than [Saudi Arabia](https://www.nytimes.com/2025/10/27/technology/saudi-arabia-ai-exporter.html). Australia is the second-largest exporter of thermal coal, has vast solar and wind resources, and the highest uranium reserves on earth. Australia is currently quite anti-nuclear at the moment, but it has no earthquakes or tsunamis to disrupt power plants. Janet Egan (CNAS) [recently called for](https://x.com/janet_e_egan/status/1950249809723740217) the development of US military AI projects in Australia, similar to the Pine Gap facility in the Northern Territory. AI safety & security research and political pressure for safety standards should focus on countries with frontier AI companies and datacenters.

## AI safety talent

Several prominent AI safety researchers have come from Australia, including Marcus Hutter, Buck Shlegeris, Jan Leike (PhD only), Ramana Kumar, Tom Everitt (PhD only), Dan Murfet, and Daniel Filan. A decent number of [MATS](https://www.matsprogram.org/) fellows come from Australia. Australian citizens can easily emigrate to the US ([E-3](https://www.uscis.gov/working-in-the-united-states/temporary-workers/e-3-specialty-occupation-workers-from-australia) visa) or UK ([YMS](https://www.gov.uk/youth-mobility) visa) for work. Seven of the [top-100 computer science universities](https://www.timeshighereducation.com/student/best-universities/best-universities-australia-computer-science-degrees) are in Australia (though admittedly none in the top-50).

## Middle power

Australia has a history as a [middle power](https://en.wikipedia.org/wiki/Middle_power) between the US and China, two of its largest trading partners. The country may play a significant role in international diplomacy efforts, such as a [superintelligence ban](https://en.wikipedia.org/wiki/Superintelligence_ban) or [international AGI project](https://www.forethought.org/research/intelsat-as-a-model-for-international-agi-governance).

## Strong environmental activism

Australia has a strong history of [environmental activism](https://en.wikipedia.org/wiki/Environmental_movement_in_Australia), which might (or [might not](https://www.lesswrong.com/posts/5nfTXn4LrxnTmBWsb/environmentalism-in-the-united-states-is-unusually-partisan)) be a useful asset for AI safety political mobilization.

## Benefits of local talent

* AI safety researchers tend to have reasonable views on AI safety and can serve as local advisors to governments, which probably trust foreign experts less.  
* Securing datacenters for SL5 to prevent proliferation of AGI is really hard and likely requires significant AI security expertise in government and local defence contractors.  
* A regulatory market approach to AI safety (possibly only useful pre-superintelligence) requires competent local auditors, standard-setters, and insurers.

# Recommendations

## Office hubs

Expand [SASS](https://sydneyaisafetyspace.org/), which is in close proximity to the new OpenAI and Anthropic offices in Sydney. Start an AI safety hub in Canberra to support the new AISI. Successful AI safety hubs have benefited from prominent founding member organizations like ARC, CG, Redwood, and MIRI (for [Constellation](https://www.constellation.org/)) and Apollo, BlueDot, and MATS (for [LISA](https://www.safeai.org.uk/)). Similarly, SASS should bring together orgs like the [Gradient Institute](https://www.gradientinstitute.org/) and [Harmony Intelligence](https://www.harmonyintelligence.com/) in a shared space, and the new Canberra hub should be built around [Good Ancestors](https://www.goodancestors.org.au/). Office hubs can benefit member orgs by providing cheaper returns to scale, hosting shared networking events, facilitating collaboration, and providing a pipeline of strong new recruits in the form of office guests and members.

## Training programs

Expand [TARA](https://www.taraprogram.org/) and the [Sydney AI Safety Fellowship](https://sasf26.com/) program, focusing on accelerating top talent and building local mentorship capacity for future programs. Don't focus on maximizing impact on participants; this is less important than reducing the mentorship bottleneck, which is best served by boosting the most advanced participants.

## Academic labs

Build relationships with AI/CS academics at UniMelb, Monash, USyd, ANU, UTS, UNSW, UA, UQ, etc. Help launch AI safety courses like Roy Rinberg and Boaz Barak did at [Harvard](https://www.lesswrong.com/posts/gcFB2RT5vpKHbH4ic/reflections-on-ta-ing-harvard-s-first-ai-safety-course). Other course inspiration is provided by [Stanford](https://web.stanford.edu/class/cs120/) and [CAIS](https://www.aisafetybook.com/virtual-course). Start AI safety academic labs like UC Berkeley CHAI, MIT AAG, NYU ARG, Bau Lab, Stanford HAI, CMU FOCAL, etc.

## Conferences

Run an annual AI safety conference like the [Australian AI Safety Forum 2024](https://aisafetyforum.au/), bringing together academia, industry, government, and nonprofit field-builders. EAGx is probably not enough, as many people from academia, industry, and government likely won't attend.
